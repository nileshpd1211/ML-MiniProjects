{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1: Constructing 5 $\\times$ 3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0864e-07, 4.5619e-41, 1.0864e-07],\n",
      "        [4.5619e-41, 1.3513e-14, 1.4917e-07],\n",
      "        [4.4859e+21, 7.2150e+22, 2.4501e-09],\n",
      "        [7.5553e+28, 5.2839e-11, 1.8888e+31],\n",
      "        [4.7414e+16, 3.1434e-12, 8.0775e+17]])\n",
      "Type of x: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)\n",
    "print('Type of x:', type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZATION of X:\n",
    "A tensor of specific data type is constructed by passing a torch.dtype to a constructor or tensor creation op.\\\n",
    "Type of x in this case is a \"Tensor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2: Randomly initialized matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2900, 0.1821, 0.6281],\n",
      "        [0.2705, 0.9885, 0.0354],\n",
      "        [0.7671, 0.6694, 0.0851],\n",
      "        [0.5777, 0.7745, 0.1435],\n",
      "        [0.9166, 0.1615, 0.3033]])\n",
      "\n",
      "\n",
      "Type of y: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(y)\n",
    "print('\\n')\n",
    "print('Type of y:', type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.rand generates a tensor variable of given input size with elements randomly sampled from values from [0,1].\\\n",
    "y has type \"Tensor\" in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1975,  0.1492,  0.8660],\n",
      "        [-0.3186,  1.9352, -0.6071],\n",
      "        [ 0.4646, -1.3051, -0.4600],\n",
      "        [-0.7308,  0.4020, -1.8410],\n",
      "        [ 0.1230, -0.4726, -0.3914]])\n",
      "\n",
      "\n",
      "Type of y: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "y = torch.randn(5, 3)\n",
    "print(y)\n",
    "print('\\n')\n",
    "print('Type of y:', type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn generates a tensor variable of given input size with elements sampled from a normal distribution\n",
    "with mean `0` and variance `1`.\n",
    "\n",
    "The difference between torch.rand() and torch.randn is the distribution elements of the tensor is sampled from. In the former case, values are randomly chosen between '[0,1]', where as in the later case it is from a normal distribution of mean '0' and variance '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0864e-07, 4.5619e-41, 1.0864e-07],\n",
      "        [4.5619e-41, 1.3513e-14, 1.4917e-07],\n",
      "        [4.4859e+21, 7.2150e+22, 2.4501e-09],\n",
      "        [7.5553e+28, 5.2839e-11, 1.8888e+31],\n",
      "        [4.7414e+16, 3.1434e-12, 8.0775e+17]], dtype=torch.float64)\n",
      "tensor([[-0.1975,  0.1492,  0.8660],\n",
      "        [-0.3186,  1.9352, -0.6071],\n",
      "        [ 0.4646, -1.3051, -0.4600],\n",
      "        [-0.7308,  0.4020, -1.8410],\n",
      "        [ 0.1230, -0.4726, -0.3914]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.double()\n",
    "y = y.double()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of x & y displayed: torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: torch.Size([5, 3])\n",
      "\n",
      "\n",
      "Shape of y: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[-0.1859, 1.3970, 0.5236],\n",
    "[ 2.3854, 0.0707, 2.1970],\n",
    "[-0.3587, 1.2359, 1.8951],\n",
    "[-0.1189, -0.1376, 0.4647],\n",
    "[-1.8968, 2.0164, 0.1092]])\n",
    "y = torch.Tensor([[ 0.4838, 0.5822, 0.2755],\n",
    "[ 1.0982, 0.4932, -0.6680],\n",
    "[ 0.7915, 0.6580, -0.5819],\n",
    "[ 0.3825, -1.1822, 1.5217],\n",
    "[ 0.6042, -0.2280, 1.3210]])\n",
    "\n",
    "print('Shape of x:', x.shape)\n",
    "print('\\n')\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of z: torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack((x, y))\n",
    "print('Shape of z:', z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1859,  1.3970,  0.5236],\n",
      "         [ 2.3854,  0.0707,  2.1970],\n",
      "         [-0.3587,  1.2359,  1.8951],\n",
      "         [-0.1189, -0.1376,  0.4647],\n",
      "         [-1.8968,  2.0164,  0.1092]],\n",
      "\n",
      "        [[ 0.4838,  0.5822,  0.2755],\n",
      "         [ 1.0982,  0.4932, -0.6680],\n",
      "         [ 0.7915,  0.6580, -0.5819],\n",
      "         [ 0.3825, -1.1822,  1.5217],\n",
      "         [ 0.6042, -0.2280,  1.3210]]])\n",
      "\n",
      "\n",
      "tensor([[-0.1859,  1.3970,  0.5236],\n",
      "        [ 2.3854,  0.0707,  2.1970],\n",
      "        [-0.3587,  1.2359,  1.8951],\n",
      "        [-0.1189, -0.1376,  0.4647],\n",
      "        [-1.8968,  2.0164,  0.1092],\n",
      "        [ 0.4838,  0.5822,  0.2755],\n",
      "        [ 1.0982,  0.4932, -0.6680],\n",
      "        [ 0.7915,  0.6580, -0.5819],\n",
      "        [ 0.3825, -1.1822,  1.5217],\n",
      "        [ 0.6042, -0.2280,  1.3210]])\n",
      "\n",
      "\n",
      "tensor([[-0.1859,  1.3970,  0.5236,  0.4838,  0.5822,  0.2755],\n",
      "        [ 2.3854,  0.0707,  2.1970,  1.0982,  0.4932, -0.6680],\n",
      "        [-0.3587,  1.2359,  1.8951,  0.7915,  0.6580, -0.5819],\n",
      "        [-0.1189, -0.1376,  0.4647,  0.3825, -1.1822,  1.5217],\n",
      "        [-1.8968,  2.0164,  0.1092,  0.6042, -0.2280,  1.3210]])\n"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "print('\\n')\n",
    "print(torch.cat((x, y), 0))\n",
    "print('\\n')\n",
    "print(torch.cat((x, y), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.stack forms a 3D tensor with individual tensors as each dimension in 3rd direction\\\n",
    "torch.cat((, ), 0) form a 2D tensor with individual tensors appended column wise (axis = 0)\\\n",
    "torch.cat((, ), 1) form a 2D tensor with individual tensors appended row wise (axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 6:Reporting value at 5th row and 3rd column in the 2d tensor y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element at the 5th row & 3rd column in 2d tensor y: tensor(1.3210)\n",
      "element at the 5th row & 3rd column in 2d tensor y accessing from z: tensor(1.3210)\n"
     ]
    }
   ],
   "source": [
    "print('element at the 5th row & 3rd column in 2d tensor y:', y[4,2])\n",
    "print('element at the 5th row & 3rd column in 2d tensor y accessing from z:', z[1,4,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here in the two methods is that when access from tensor 'z' we need to give all three indexes as it is a 3D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements corresponding to the 5th row and 3rd column in z: tensor([0.1092, 1.3210])\n",
      "No. of elements corresponding to 5th row and 3rd column in z: 2\n"
     ]
    }
   ],
   "source": [
    "print('Elements corresponding to the 5th row and 3rd column in z:', z[:,4,2])\n",
    "print('No. of elements corresponding to 5th row and 3rd column in z:', len(z[:,4,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 8: Adding two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2979,  1.9792,  0.7991],\n",
      "        [ 3.4836,  0.5639,  1.5290],\n",
      "        [ 0.4328,  1.8939,  1.3132],\n",
      "        [ 0.2636, -1.3198,  1.9864],\n",
      "        [-1.2926,  1.7884,  1.4302]])\n",
      "tensor([[ 0.2979,  1.9792,  0.7991],\n",
      "        [ 3.4836,  0.5639,  1.5290],\n",
      "        [ 0.4328,  1.8939,  1.3132],\n",
      "        [ 0.2636, -1.3198,  1.9864],\n",
      "        [-1.2926,  1.7884,  1.4302]])\n",
      "tensor([[ 0.2979,  1.9792,  0.7991],\n",
      "        [ 3.4836,  0.5639,  1.5290],\n",
      "        [ 0.4328,  1.8939,  1.3132],\n",
      "        [ 0.2636, -1.3198,  1.9864],\n",
      "        [-1.2926,  1.7884,  1.4302]])\n",
      "tensor([[ 0.2979,  1.9792,  0.7991],\n",
      "        [ 3.4836,  0.5639,  1.5290],\n",
      "        [ 0.4328,  1.8939,  1.3132],\n",
      "        [ 0.2636, -1.3198,  1.9864],\n",
      "        [-1.2926,  1.7884,  1.4302]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y) \n",
    "print(torch.add(x, y)) \n",
    "print(x.add(y))\n",
    "torch.add(x, y, out=x) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x + y, torch.add(x, y), x.add(y), torch.add(x, y, out=x)\\\n",
    "All the four methods gave the same similar results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".view function reshapes the input vector in the shape given as input\\\n",
    "for example, in x.view(16), it reshapes (4,4) x tensor into (16,1) tensor\\\n",
    "-1 accounts for all left dimension size given other dimension sizes, eg. In x.view(-1,8), -1 accounts for 2 as 2x8=4x4\n",
    "[conservation of elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-20.0822,  18.0548]])\n",
      "Shape of torch.mm(x,y) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10,10)\n",
    "y = torch.randn(2,100)\n",
    "x = x.view(1,-1)\n",
    "y = y.view(-1,2)\n",
    "print(torch.mm(x,y))\n",
    "print('Shape of torch.mm(x,y)', torch.mm(x,y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 11: Convertion between Torch tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "\n",
      "\n",
      "Type of a: <class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "Type of b: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print('\\n')\n",
    "print('Type of a:', type(a))\n",
    "print('\\n')\n",
    "print('Type of b:', type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of a: torch.tensor\\\n",
    "Type of b: numpy.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([2., 1., 1., 1., 1.])\n",
      "b: [2. 1. 1. 1. 1.]\n",
      "Address of torch tensor a 0x7f2a6744bab0\n",
      "\n",
      "\n",
      "Address of numpy array b 0x7f2a6744d030\n"
     ]
    }
   ],
   "source": [
    "a[0] += 1\n",
    "print('a:',a)\n",
    "print('b:',b)\n",
    "\n",
    "print('Address of torch tensor a', hex(id(a)))\n",
    "print('\\n')\n",
    "print('Address of numpy array b', hex(id(b)))   #Check if memory same or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 1 to the first element tensor of Tensor 'a' automatically changed the 'b' numpy array\\\n",
    "They do not share their underlying memory locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 13:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([2., 2., 2., 2., 2.])\n",
      "b: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "a.add_(1)\n",
    "print('a:',a)\n",
    "print('b:',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.add_(1) adds 1 to each element of the tensor a\\\n",
    "Numpy array b also gets updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([2., 2., 2., 2., 2.])\n",
      "b: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "a[:] += 1\n",
    "print('a:',a)\n",
    "print('b:',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a[:]+=1 adds 1 to each element of the tensor a\\\n",
    "Numpy array b also gets updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([2., 2., 2., 2., 2.])\n",
      "b: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "a = a.add(1)\n",
    "print('a:',a)\n",
    "print('b:',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = a.add(1) adds one to each element of the tensor a\\\n",
    "No effect on numpy array b. Values of b remain unchanged\\\n",
    "\n",
    "a.add_(1) and a[:]+=1 add one to each element of tensor 'a', meanwhile values of numpy array b also get updated.\\\n",
    "Whereas a.add(1) updates only the values of tensor 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [2. 2. 2. 2. 2.]\n",
      "b: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print('a:',a)\n",
    "print('b:',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we obtained tensor 'b' converted from numpy array 'a'.\\\n",
    "using np.add(a,1,out=a) updates both the values of tensor 'b' and numpy array 'a'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Duration taken by .to(device) method: 0:00:03.324070\n",
      "Duration taken by device = device method: 0:00:00.000399\n",
      "tensor([[ 0.3698, -1.7194,  1.1955],\n",
      "        [-1.9334,  0.8977,  0.2621],\n",
      "        [-1.0172, -1.1353,  0.0452],\n",
      "        [ 1.3237, -0.6606, -0.0390],\n",
      "        [-1.5129,  0.4130, -0.8862]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "x = torch.randn(5, 3).to(device)# do your work here\n",
    "end_time = datetime.now()\n",
    "print('Duration taken by .to(device) method: {}'.format(end_time - start_time))\n",
    "\n",
    "start_time = datetime.now()\n",
    "y = torch.randn(5, 3, device=device)\n",
    "end_time = datetime.now()\n",
    "print('Duration taken by device = device method: {}'.format(end_time - start_time))\n",
    "\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we check if gpu is available. If it is available, 'cuda' is saved in 'device' variable else 'cpu' is used.\\\n",
    "torch.cuda.is_available() checks if gpu allocation is available for computation.\\\n",
    "We confirm the availivity by printing the device variable stored.\\\n",
    ".to(device) or (,device = device) is used to used to move the tensor to the available device memory.\n",
    "\n",
    "Comparing the time taken by the two operations for moving the tensor into gpu's memory, using y=torch.randn(5, 3, device=device) seems more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3697798  -1.719365    1.1955328 ]\n",
      " [-1.9333512   0.8977027   0.26212847]\n",
      " [-1.0171719  -1.1352932   0.04522318]\n",
      " [ 1.3237206  -0.6606356  -0.03904295]\n",
      " [-1.512912    0.41303235 -0.8862239 ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-11d5c486e6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "print(z.cpu().numpy())\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU and CPU both have their separate memory allocation.\\\n",
    ".numpy() shares memory with the CPU tensor. In the last cell, we moved tensor z to gpu memory.\\\n",
    "Hence, print(z.cpu().numpy()) worked as first the tensor z was transfer to cpu memory using .cpu().\\\n",
    "The second statement showed error as, .numpy() could't access gpu memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 17: Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "requires_grad attribute of y: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print('requires_grad attribute of y:', y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As y resulted from x, it automatically has requires_grad = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad attribute of y: None\n",
      "Grad attribute of x: None\n"
     ]
    }
   ],
   "source": [
    "print('Grad attribute of y:',y.grad)\n",
    "print('Grad attribute of x:',x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "f = z.mean()\n",
    "print(z, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "    x_{1}       & x_{2}  \\\\\n",
    "    x_{3}       & x_{4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Y=\n",
    "\\begin{bmatrix}\n",
    "    x_{1}+2      & x_{2}+2  \\\\\n",
    "    x_{3}+2     & x_{4}+2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Z=\n",
    "\\begin{bmatrix}\n",
    "    3(x_{1}+2)^{2}      & 3(x_{2}+2)^{2}  \\\\\n",
    "    3(x_{3}+2)^{2}    & 3(x_{4}+2)^{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_{1},x_{2},x_{3},x_{4})= \\frac{3(x_{1}+2)^{2}+3(x_{2}+2)^{2}+3(x_{3}+2)^{2}+3(x_{4}+2)^{2}}{4}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward() # That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta f}{\\delta x}$ is calculated in the above x.grad operation. The output is a matrix corresponding to each $x_{ij}$\\\n",
    "\\begin{equation}\n",
    "\\frac{\\delta f}{\\delta x} = \n",
    "\\frac{1}{4}\n",
    "\\begin{bmatrix}\n",
    "    6(x_{1}+2)      & 6(x_{2}+2)  \\\\\n",
    "    6(x_{3}+2)    & 6(x_{4}+2) \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "(\\nabla_{x} f(x))_{i} = \\frac{6(x_{i}+2)}{4}\n",
    "\\end{equation}\n",
    "\n",
    "Putting values $x_{i} = 1$ for all i's , we get \n",
    "\\begin{bmatrix}\n",
    "    4.5      & 4.5  \\\\\n",
    "    4.5    & 4.5 \\\\\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 21:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import MNISTtools\n",
    "xtrain,ltrain = MNISTtools.load(dataset=\"training\", path=\"/datasets/MNIST/\")\n",
    "xtest, ltest = MNISTtools.load(dataset=\"testing\", path=\"/datasets/MNIST/\")\n",
    "\n",
    "def normalize_MNIST_images(x):\n",
    "  return ((x-127.5)/127.5).astype(np.float64)\n",
    "\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "xtest = normalize_MNIST_images(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(28,28,1,60000)\n",
    "xtest = xtest.reshape(28,28,1,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set dimension: (60000, 1, 28, 28)\n",
      "testing set dimension: (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "xtrain = np.moveaxis(xtrain,[0,1,2,3],[2,3,1,0])\n",
    "xtest = np.moveaxis(xtest,[0,1,2,3],[2,3,1,0])\n",
    "\n",
    "print('training set dimension:',xtrain.shape)\n",
    "print('testing set dimension:',xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 23:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 7\n"
     ]
    }
   ],
   "source": [
    "#Verification\n",
    "MNISTtools.show(xtrain[42, 0, :, :])\n",
    "print('True label:', ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 24:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = torch.from_numpy(xtrain)\n",
    "ltrain = torch.from_numpy(ltrain)\n",
    "xtest = torch.from_numpy(xtest)\n",
    "ltest = torch.from_numpy(ltest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 25:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dimensions after\\\n",
    "(i) First convolution layer: 6x24x24\\\n",
    "(ii) First Maxpool layer: 6x12x12\\\n",
    "(iii) Second convolution layer: 16x8x8\\\n",
    "(iv) Second Maxpool layer: 16x4x4\n",
    "\n",
    "Output after second maxpool layer is flattened and given as input to the fully connected layer (v).\\\n",
    "Hence, dimension = $16*4*4 = 256$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 26:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# This is our neural networks class that inherits from nn.Module\n",
    "class LeNet(nn.Module):\n",
    "# Here we define our network structure\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5).double()\n",
    "        self.conv2 = nn.Conv2d(6,16,5).double()\n",
    "        self.fc1 = nn.Linear(16*4*4,120).double()\n",
    "        self.fc2 = nn.Linear(120, 84).double()\n",
    "        self.fc3 = nn.Linear(84, 10).double()\n",
    "        # Here we define one forward pass through the network\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)).double()\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2)).double()\n",
    "        x = x.view(-1, self.num_flat_features(x)).double()\n",
    "        x =  F.relu(self.fc1(x)).double()\n",
    "        x = F.relu(self.fc2(x)).double()\n",
    "        x = self.fc3(x).double()\n",
    "        return x\n",
    "    # Determine the number of features in a batch of tensors\n",
    "    def num_flat_features(self, x ):\n",
    "        size = x.size()[1:]\n",
    "        return np.prod(size)\n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 27:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5]) True\n",
      "conv1.bias torch.Size([6]) True\n",
      "conv2.weight torch.Size([16, 6, 5, 5]) True\n",
      "conv2.bias torch.Size([16]) True\n",
      "fc1.weight torch.Size([120, 256]) True\n",
      "fc1.bias torch.Size([120]) True\n",
      "fc2.weight torch.Size([84, 120]) True\n",
      "fc2.bias torch.Size([84]) True\n",
      "fc3.weight torch.Size([10, 84]) True\n",
      "fc3.bias torch.Size([10]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnable parameter:\n",
    "1. For convolution layer 1, weights corresponding to $6 [5x5] $ filter and  $6 $ bias terms are learnable.\n",
    "2. For convolution layer 2, weights corresponding to  $16 [5x5] $ filter and  $16 $ bias terms are learnable.\n",
    "3. For fully connected layer 1, weights corresponding to weight matrix of dimension $[256x120]$ is learnable.\n",
    "4. For fully connected layer 2, weights corresponding to weight matrix of dimension $[120x84]$ is learnable.\n",
    "5. For fully connected layer 3, weights corresponding to weight matrix of dimension $[84x10]$ is learnable.\n",
    "\n",
    "All learnable parameters returned True in return to param.requies_grad.\\\n",
    "Hence, gradient of all learnable parameters are being tracked\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yinit = net(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.8200)\n"
     ]
    }
   ],
   "source": [
    "_, lpred = yinit.max(1)\n",
    "print(100 * (ltest == lpred).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is low which is expected as no training has been done yet!\\\n",
    "We just forward passed the inputs to the randomly initialized weights and obtained the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 29 and 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.301\n",
      "[1,   200] loss: 2.289\n",
      "[1,   300] loss: 2.274\n",
      "[1,   400] loss: 2.249\n",
      "[1,   500] loss: 2.191\n",
      "[1,   600] loss: 1.980\n",
      "[2,   100] loss: 1.289\n",
      "[2,   200] loss: 0.682\n",
      "[2,   300] loss: 0.519\n",
      "[2,   400] loss: 0.415\n",
      "[2,   500] loss: 0.378\n",
      "[2,   600] loss: 0.334\n",
      "[3,   100] loss: 0.304\n",
      "[3,   200] loss: 0.294\n",
      "[3,   300] loss: 0.272\n",
      "[3,   400] loss: 0.256\n",
      "[3,   500] loss: 0.245\n",
      "[3,   600] loss: 0.220\n",
      "Time Taken: 398.8633885383606\n"
     ]
    }
   ],
   "source": [
    "def backprop_deep(xtrain, ltrain, net, T, B=100, gamma=.001, rho=.9):\n",
    "    \n",
    "    N = xtrain.size()[0] # Training set size\n",
    "    NB =xtrain.shape[0]//B # Number of minibatches\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=gamma, momentum=rho)\n",
    "\n",
    "    for epoch in range(T):\n",
    "        running_loss = 0.0\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for k in range(NB):\n",
    "        # Extract k-th minibatch from xtrain and ltrain\n",
    "            minibatch_indices = shuffled_indices[B*k:min(B*(k+1), N)]\n",
    "            inputs = xtrain[minibatch_indices]\n",
    "            labels = ltrain[minibatch_indices]\n",
    "\n",
    "            # Initialize the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            # Forward propagation\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            # Error evaluation\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Back propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Parameter update\n",
    "            optimizer.step()\n",
    "            # Print averaged loss per minibatch every 100 mini-batches\n",
    "            # Compute and print statistics\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if k % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %(epoch + 1, k + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "net = LeNet()     \n",
    "xtrain = xtrain.to('cpu')\n",
    "ltrain = ltrain.to('cpu')\n",
    "xtest = xtrain.to('cpu')\n",
    "ltest = ltrain.to('cpu')\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "backprop_deep(xtrain, ltrain, net, T=3)\n",
    "end = time.time()\n",
    "\n",
    "print('Time Taken:',end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93.5700)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    yinit = net(xtest)\n",
    "_, lpred = yinit.max(1)\n",
    "print(100 * (ltest == lpred).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training for 3 epoch we got an accuracy of $93.5700 \\%$\\\n",
    "Before training, the accuracy on $11.82 \\%$\n",
    "\n",
    "A huge improve is seen after training just 3 epochs.\\\n",
    "Now lets compare the training time and accuracy after moving the model to gpu memory and use gpu for faster computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet().to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to transfer all the tensors to gpu memory before we can pass it to the model architure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.305\n",
      "[1,   200] loss: 2.299\n",
      "[1,   300] loss: 2.292\n",
      "[1,   400] loss: 2.283\n",
      "[1,   500] loss: 2.268\n",
      "[1,   600] loss: 2.232\n",
      "[2,   100] loss: 2.136\n",
      "[2,   200] loss: 1.838\n",
      "[2,   300] loss: 1.225\n",
      "[2,   400] loss: 0.695\n",
      "[2,   500] loss: 0.505\n",
      "[2,   600] loss: 0.429\n",
      "[3,   100] loss: 0.373\n",
      "[3,   200] loss: 0.321\n",
      "[3,   300] loss: 0.314\n",
      "[3,   400] loss: 0.266\n",
      "[3,   500] loss: 0.242\n",
      "[3,   600] loss: 0.230\n",
      "[4,   100] loss: 0.218\n",
      "[4,   200] loss: 0.207\n",
      "[4,   300] loss: 0.192\n",
      "[4,   400] loss: 0.184\n",
      "[4,   500] loss: 0.178\n",
      "[4,   600] loss: 0.174\n",
      "[5,   100] loss: 0.158\n",
      "[5,   200] loss: 0.152\n",
      "[5,   300] loss: 0.147\n",
      "[5,   400] loss: 0.149\n",
      "[5,   500] loss: 0.132\n",
      "[5,   600] loss: 0.126\n",
      "[6,   100] loss: 0.122\n",
      "[6,   200] loss: 0.129\n",
      "[6,   300] loss: 0.118\n",
      "[6,   400] loss: 0.114\n",
      "[6,   500] loss: 0.107\n",
      "[6,   600] loss: 0.117\n",
      "[7,   100] loss: 0.104\n",
      "[7,   200] loss: 0.106\n",
      "[7,   300] loss: 0.104\n",
      "[7,   400] loss: 0.104\n",
      "[7,   500] loss: 0.095\n",
      "[7,   600] loss: 0.097\n",
      "[8,   100] loss: 0.085\n",
      "[8,   200] loss: 0.089\n",
      "[8,   300] loss: 0.090\n",
      "[8,   400] loss: 0.092\n",
      "[8,   500] loss: 0.098\n",
      "[8,   600] loss: 0.087\n",
      "[9,   100] loss: 0.084\n",
      "[9,   200] loss: 0.083\n",
      "[9,   300] loss: 0.085\n",
      "[9,   400] loss: 0.078\n",
      "[9,   500] loss: 0.077\n",
      "[9,   600] loss: 0.082\n",
      "[10,   100] loss: 0.076\n",
      "[10,   200] loss: 0.068\n",
      "[10,   300] loss: 0.075\n",
      "[10,   400] loss: 0.082\n",
      "[10,   500] loss: 0.074\n",
      "[10,   600] loss: 0.073\n",
      "Time Taken: 23.438609838485718\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "xtrain = xtrain.to('cuda')\n",
    "ltrain = ltrain.to('cuda')\n",
    "xtest = xtrain.to('cuda')\n",
    "ltest = ltrain.to('cuda')\n",
    "backprop_deep(xtrain, ltrain, net, T=10)\n",
    "end = time.time()\n",
    "print('Time Taken:',end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 33:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(97.7583, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    yinit = net(xtest)\n",
    "_, lpred = yinit.max(1)\n",
    "acc_gpu = 100 * (ltest == lpred).float().mean()\n",
    "print((acc_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training for 3 epoch we got an accuracy of $97.7583 \\%$\\\n",
    "In the last method, the accuracy on $93.5700 \\%$\n",
    "\n",
    "Increase in test accuracy is seen after training for 10 epochs. Also, the computation was very fast due to the \n",
    "utilization of gpu computation. We also compare the time taken when GPU and CPU is used for training.\\\n",
    "In case of 3 Epochs and CPU used for training : 398.8633885383606 s taken\\\n",
    "In case of 10 Epochs and GPU used for training: 23.438609838485718 s taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
